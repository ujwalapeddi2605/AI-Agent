{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3j3iZQvoAvx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Load IMDb dataset word index (to simulate tokenizer)\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {v+3: k for k, v in word_index.items()}\n",
        "word_index = {k: (v+3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# Load pretrained model (or build one if needed)\n",
        "# For demo, we'll build a quick one below. You can skip this block if you have a model already.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "x_train = pad_sequences(x_train, maxlen=256, padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=256, padding='post')\n",
        "\n",
        "# Build and train a simple model (train once and save)\n",
        "model = Sequential([\n",
        "    Embedding(10000, 16),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=2, batch_size=512, validation_split=0.2)\n",
        "model.save(\"movie_review_model.h5\")\n",
        "\n",
        "# Load model (use this part once saved)\n",
        "model = load_model(\"movie_review_model.h5\")\n",
        "\n",
        "# Function to convert review text to sequence\n",
        "def review_to_sequence(text):\n",
        "    tokens = text.lower().split()\n",
        "    seq = [word_index.get(word, 2) for word in tokens]\n",
        "    return pad_sequences([seq], maxlen=256, padding='post')\n",
        "\n",
        "# Prediction function\n",
        "def predict_review(text):\n",
        "    sequence = review_to_sequence(text)\n",
        "    prediction = model.predict(sequence)[0][0]\n",
        "    return \"Hit ðŸŽ¬\" if prediction > 0.5 else \"Flop ðŸ’£\"\n",
        "\n",
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    review = input(\"Enter a movie review: \")\n",
        "    result = predict_review(review)\n",
        "    print(\"Prediction:\", result)\n"
      ]
    }
  ]
}